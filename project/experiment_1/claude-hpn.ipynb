{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LoRAGenerator(nn.Module):\n",
    "    def __init__(self, hidden_size, lora_rank):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lora_rank = lora_rank\n",
    "        \n",
    "        # v_proj uses 512 output dimension (as seen in weight shape)\n",
    "        self.v_proj_out_dim = 512\n",
    "        \n",
    "        self.context_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_size,\n",
    "                nhead=8,\n",
    "                dim_feedforward=4*hidden_size,\n",
    "                dropout=0.1,\n",
    "                activation=\"gelu\",\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        # Separate generators for q_proj and v_proj\n",
    "        self.q_proj_generator = self._create_lora_generator(hidden_size)  # 2048 output dim\n",
    "        self.v_proj_generator = self._create_lora_generator(self.v_proj_out_dim)  # 512 output dim\n",
    "        \n",
    "    def _create_lora_generator(self, out_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, 4*self.hidden_size),\n",
    "            nn.GELU(),\n",
    "            # Output matches dimensions needed for A and B separately\n",
    "            nn.Linear(4*self.hidden_size, self.lora_rank * self.hidden_size + out_dim * self.lora_rank)\n",
    "        )\n",
    "    \n",
    "    def _reshape_lora_matrices(self, raw_output, out_dim):\n",
    "        # Split the output into parts for A and B\n",
    "        split_point = self.lora_rank * self.hidden_size\n",
    "        a_flat = raw_output[:, :split_point]\n",
    "        b_flat = raw_output[:, split_point:]\n",
    "        \n",
    "        # Reshape A to [batch, rank, hidden]\n",
    "        lora_A = a_flat.view(-1, self.lora_rank, self.hidden_size)\n",
    "        \n",
    "        # Reshape B to [batch, out_dim, rank]\n",
    "        lora_B = b_flat.view(-1, out_dim, self.lora_rank)\n",
    "        \n",
    "        return lora_A, lora_B\n",
    "    \n",
    "    def forward(self, context_embeddings):\n",
    "        # Process context\n",
    "        context_encoded = self.context_encoder(context_embeddings)\n",
    "        context_pooled = context_encoded.mean(dim=1)  # [batch_size, hidden_size]\n",
    "        \n",
    "        updates = {}\n",
    "        \n",
    "        # Generate q_proj updates (2048 output dim)\n",
    "        q_raw = self.q_proj_generator(context_pooled)\n",
    "        q_lora_A, q_lora_B = self._reshape_lora_matrices(q_raw, self.hidden_size)\n",
    "        updates['q_proj'] = {\n",
    "            'lora_A': q_lora_A,\n",
    "            'lora_B': q_lora_B\n",
    "        }\n",
    "        \n",
    "        # Generate v_proj updates (512 output dim)\n",
    "        v_raw = self.v_proj_generator(context_pooled)\n",
    "        v_lora_A, v_lora_B = self._reshape_lora_matrices(v_raw, self.v_proj_out_dim)\n",
    "        updates['v_proj'] = {\n",
    "            'lora_A': v_lora_A,\n",
    "            'lora_B': v_lora_B\n",
    "        }\n",
    "        \n",
    "        return updates\n",
    "\n",
    "# def train_lora_generator(\n",
    "#     base_model,\n",
    "#     lora_model,\n",
    "#     generator,\n",
    "#     train_dataloader,\n",
    "#     num_epochs=5,\n",
    "#     learning_rate=1e-4,\n",
    "# ):\n",
    "#     device = next(base_model.parameters()).device\n",
    "#     generator = generator.to(device)\n",
    "#     optimizer = torch.optim.AdamW(generator.parameters(), lr=learning_rate)\n",
    "    \n",
    "#     # Get target modules\n",
    "#     target_modules = {}\n",
    "#     for name, module in lora_model.named_modules():\n",
    "#         if hasattr(module, 'lora_A'):\n",
    "#             if 'q_proj' in name or 'v_proj' in name:\n",
    "#                 target_modules[name] = module\n",
    "#                 print(f\"\\nFound module {name}\")\n",
    "#                 print(f\"LoRA A shape: {module.lora_A[module.active_adapter].weight.shape}\")\n",
    "#                 print(f\"LoRA B shape: {module.lora_B[module.active_adapter].weight.shape}\")\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         total_loss = 0\n",
    "#         num_batches = 0\n",
    "        \n",
    "#         for batch_idx, batch in enumerate(train_dataloader):\n",
    "#             context_ids = batch[\"context\"].to(device)\n",
    "#             target_ids = batch[\"target\"].to(device)\n",
    "            \n",
    "#             # Generate base model outputs\n",
    "#             with torch.no_grad():\n",
    "#                 context_embeddings = base_model.model.embed_tokens(context_ids)\n",
    "#                 base_outputs = base_model(input_ids=context_ids)\n",
    "#                 base_logits = base_outputs.logits\n",
    "            \n",
    "#             # Generate LoRA updates\n",
    "#             lora_updates = generator(context_embeddings)\n",
    "            \n",
    "#             # Apply updates\n",
    "#             for module_name, update in lora_updates.items():\n",
    "#                 for full_name, module in target_modules.items():\n",
    "#                     if module_name in full_name:\n",
    "#                         lora_A = update['lora_A'].mean(dim=0)\n",
    "#                         lora_B = update['lora_B'].mean(dim=0)\n",
    "                        \n",
    "#                         # Print shapes for debugging\n",
    "#                         if batch_idx == 0:\n",
    "#                             print(f\"\\nUpdating {full_name}\")\n",
    "#                             print(f\"Generated LoRA A shape: {lora_A.shape}\")\n",
    "#                             print(f\"Target LoRA A shape: {module.lora_A[module.active_adapter].weight.shape}\")\n",
    "#                             print(f\"Generated LoRA B shape: {lora_B.shape}\")\n",
    "#                             print(f\"Target LoRA B shape: {module.lora_B[module.active_adapter].weight.shape}\")\n",
    "                        \n",
    "#                         # Update weights\n",
    "#                         module.lora_A[module.active_adapter].weight.data = lora_A\n",
    "#                         module.lora_B[module.active_adapter].weight.data = lora_B\n",
    "            \n",
    "#             # Forward pass with updated LoRA model\n",
    "#             lora_outputs = lora_model(input_ids=target_ids)\n",
    "            \n",
    "#             # Compute loss\n",
    "#             loss = F.kl_div(\n",
    "#                 F.log_softmax(lora_outputs.logits, dim=-1),\n",
    "#                 F.softmax(base_logits, dim=-1),\n",
    "#                 reduction='batchmean'\n",
    "#             )\n",
    "            \n",
    "#             # Optimization\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             total_loss += loss.item()\n",
    "#             num_batches += 1\n",
    "            \n",
    "#             if num_batches % 1 == 0:\n",
    "#                 print(f\"Epoch {epoch}, Batch {num_batches}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "#         avg_loss = total_loss / num_batches\n",
    "#         print(f\"Epoch {epoch} completed. Average loss: {avg_loss:.4f}\")\n",
    "# def train_lora_generator(\n",
    "#     base_model,\n",
    "#     lora_model,\n",
    "#     generator,\n",
    "#     train_dataloader,\n",
    "#     num_epochs=5,\n",
    "#     learning_rate=1e-4,\n",
    "# ):\n",
    "#     device = next(base_model.parameters()).device\n",
    "#     generator = generator.to(device)\n",
    "#     optimizer = torch.optim.AdamW(generator.parameters(), lr=learning_rate)\n",
    "    \n",
    "#     # Initialize target modules\n",
    "#     target_modules = {}\n",
    "#     for name, module in lora_model.named_modules():\n",
    "#         if hasattr(module, 'lora_A'):\n",
    "#             if 'q_proj' in name or 'v_proj' in name:\n",
    "#                 target_modules[name] = module\n",
    "    \n",
    "#     print(f\"Found {len(target_modules)} target modules for LoRA updates\")\n",
    "    \n",
    "#     # Track statistics\n",
    "#     epoch_stats = []\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "#         print(\"-\" * 20)\n",
    "        \n",
    "#         total_loss = 0\n",
    "#         batch_losses = []\n",
    "#         num_batches = 0\n",
    "        \n",
    "#         for batch_idx, batch in enumerate(train_dataloader):\n",
    "#             context_ids = batch[\"context\"].to(device)\n",
    "#             target_ids = batch[\"target\"].to(device)\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 context_embeddings = base_model.model.embed_tokens(context_ids)\n",
    "#                 base_outputs = base_model(input_ids=context_ids)\n",
    "#                 base_logits = base_outputs.logits\n",
    "            \n",
    "#             # Generate and apply LoRA updates\n",
    "#             lora_updates = generator(context_embeddings)\n",
    "            \n",
    "#             # Update LoRA weights\n",
    "#             for module_name, update in lora_updates.items():\n",
    "#                 for full_name, module in target_modules.items():\n",
    "#                     if module_name in full_name:\n",
    "#                         lora_A = update['lora_A'].mean(dim=0)\n",
    "#                         lora_B = update['lora_B'].mean(dim=0)\n",
    "#                         module.lora_A[module.active_adapter].weight.data = lora_A\n",
    "#                         module.lora_B[module.active_adapter].weight.data = lora_B\n",
    "            \n",
    "#             # Forward pass with updated LoRA model\n",
    "#             lora_outputs = lora_model(input_ids=target_ids)\n",
    "            \n",
    "#             # Compute loss\n",
    "#             loss = F.kl_div(\n",
    "#                 F.log_softmax(lora_outputs.logits, dim=-1),\n",
    "#                 F.softmax(base_logits, dim=-1),\n",
    "#                 reduction='batchmean'\n",
    "#             )\n",
    "\n",
    "#             #overpenalizing -- teacher forcing\n",
    "            \n",
    "#             # Optimization\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             # Track statistics\n",
    "#             current_loss = loss.item()\n",
    "#             total_loss += current_loss\n",
    "#             batch_losses.append(current_loss)\n",
    "#             num_batches += 1\n",
    "            \n",
    "#             print(f\"Batch {batch_idx+1}: Loss = {current_loss:.4f}\")\n",
    "        \n",
    "#         # Compute epoch statistics\n",
    "#         avg_loss = total_loss / num_batches\n",
    "#         min_loss = min(batch_losses)\n",
    "#         max_loss = max(batch_losses)\n",
    "        \n",
    "#         stats = {\n",
    "#             'epoch': epoch + 1,\n",
    "#             'avg_loss': avg_loss,\n",
    "#             'min_loss': min_loss,\n",
    "#             'max_loss': max_loss,\n",
    "#             'num_batches': num_batches\n",
    "#         }\n",
    "#         epoch_stats.append(stats)\n",
    "        \n",
    "#         print(\"\\nEpoch Statistics:\")\n",
    "#         print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "#         print(f\"Min Loss: {min_loss:.4f}\")\n",
    "#         print(f\"Max Loss: {max_loss:.4f}\")\n",
    "#         print(f\"Number of Batches: {num_batches}\")\n",
    "    \n",
    "#     # Print final summary\n",
    "#     print(\"\\nTraining Summary\")\n",
    "#     print(\"=\" * 40)\n",
    "#     print(\"Epoch  |  Avg Loss  |  Min Loss  |  Max Loss\")\n",
    "#     print(\"-\" * 40)\n",
    "#     for stats in epoch_stats:\n",
    "#         print(f\"{stats['epoch']:5d}  |  {stats['avg_loss']:.4f}    |  {stats['min_loss']:.4f}    |  {stats['max_loss']:.4f}\")\n",
    "# # Model setup\n",
    "\n",
    "\n",
    "def autoregressive_teacher_forcing(base_model, lora_model, input_ids, max_steps=20):\n",
    "    \"\"\"\n",
    "    Run step-by-step teacher forcing evaluation:\n",
    "    1. Get base model's next token prediction\n",
    "    2. Compute KL divergence of LoRA model on that specific token\n",
    "    3. Feed the base model's token back to LoRA model for next step\n",
    "    \"\"\"\n",
    "    device = input_ids.device\n",
    "    batch_size = input_ids.shape[0]\n",
    "    losses = []\n",
    "    \n",
    "    # Start with the input context\n",
    "    lora_tokens = input_ids.clone()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step in range(max_steps):\n",
    "            # Get base model prediction\n",
    "            base_outputs = base_model(input_ids=lora_tokens)\n",
    "            base_logits = base_outputs.logits[:, -1, :]  # Last token predictions\n",
    "            base_probs = F.softmax(base_logits, dim=-1)\n",
    "            \n",
    "            # Get base model's token choice\n",
    "            base_next_token = base_logits.argmax(dim=-1)  # [batch_size]\n",
    "            \n",
    "            # Get LoRA model prediction (on same sequence)\n",
    "            lora_outputs = lora_model(input_ids=lora_tokens)\n",
    "            lora_logits = lora_outputs.logits[:, -1, :]  # Last token predictions\n",
    "            lora_log_probs = F.log_softmax(lora_logits, dim=-1)\n",
    "            \n",
    "            # Compute KL divergence just for this step's token\n",
    "            step_loss = F.kl_div(\n",
    "                lora_log_probs.unsqueeze(1),  # [batch, 1, vocab]\n",
    "                base_probs.unsqueeze(1),      # [batch, 1, vocab]\n",
    "                reduction='none'\n",
    "            ).sum(dim=-1)  # [batch, 1]\n",
    "            \n",
    "            losses.append(step_loss)\n",
    "            \n",
    "            # Update LoRA tokens with base model's choice\n",
    "            lora_tokens = torch.cat([lora_tokens, base_next_token.unsqueeze(1)], dim=1)\n",
    "    \n",
    "    # Stack losses for all steps\n",
    "    step_losses = torch.cat(losses, dim=1)  # [batch_size, max_steps]\n",
    "    return step_losses, lora_tokens\n",
    "\n",
    "def train_lora_generator(\n",
    "    base_model,\n",
    "    lora_model,\n",
    "    generator,\n",
    "    train_dataloader,\n",
    "    num_epochs=5,\n",
    "    learning_rate=1e-4,\n",
    "    max_gen_steps=20\n",
    "):\n",
    "    device = next(base_model.parameters()).device\n",
    "    generator = generator.to(device)\n",
    "    optimizer = torch.optim.AdamW(generator.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initialize target modules\n",
    "    target_modules = {}\n",
    "    for name, module in lora_model.named_modules():\n",
    "        if hasattr(module, 'lora_A'):\n",
    "            if 'q_proj' in name or 'v_proj' in name:\n",
    "                target_modules[name] = module\n",
    "    \n",
    "    print(f\"Found {len(target_modules)} target modules for LoRA updates\")\n",
    "    \n",
    "    # Track statistics\n",
    "    epoch_stats = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_steps = 0\n",
    "        batch_losses = []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            context_ids = batch[\"context\"].to(device)\n",
    "            \n",
    "            # Get initial context embeddings for LoRA update generation\n",
    "            with torch.no_grad():\n",
    "                context_embeddings = base_model.model.embed_tokens(context_ids)\n",
    "            \n",
    "            # Generate and apply LoRA updates based on context\n",
    "            lora_updates = generator(context_embeddings)\n",
    "            \n",
    "            # Update LoRA weights\n",
    "            for module_name, update in lora_updates.items():\n",
    "                for full_name, module in target_modules.items():\n",
    "                    if module_name in full_name:\n",
    "                        lora_A = update['lora_A'].mean(dim=0)\n",
    "                        lora_B = update['lora_B'].mean(dim=0)\n",
    "                        module.lora_A[module.active_adapter].weight.data = lora_A\n",
    "                        module.lora_B[module.active_adapter].weight.data = lora_B\n",
    "            \n",
    "            # Run teacher forcing evaluation\n",
    "            step_losses, generated_tokens = autoregressive_teacher_forcing(\n",
    "                base_model=base_model,\n",
    "                lora_model=lora_model,\n",
    "                input_ids=context_ids,\n",
    "                max_steps=max_gen_steps\n",
    "            )\n",
    "            \n",
    "            # Compute mean loss over steps\n",
    "            loss = step_losses.mean()\n",
    "            \n",
    "            # Print step-by-step losses for first batch\n",
    "            if batch_idx == 0:\n",
    "                print(\"\\nStep-by-step losses for first sequence:\")\n",
    "                for step, step_loss in enumerate(step_losses[0]):\n",
    "                    print(f\"Step {step}: Loss = {step_loss.item():.4f}\")\n",
    "                \n",
    "                print(\"\\nGenerated text:\")\n",
    "                print(\"Context:\", tokenizer.decode(context_ids[0]))\n",
    "                print(\"Generated:\", tokenizer.decode(generated_tokens[0]))\n",
    "            \n",
    "            # Optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track statistics\n",
    "            current_loss = loss.item()\n",
    "            total_loss += current_loss * len(step_losses)\n",
    "            total_steps += len(step_losses)\n",
    "            batch_losses.append(current_loss)\n",
    "            \n",
    "            print(f\"Batch {batch_idx+1}: Average Loss = {current_loss:.4f}\")\n",
    "        \n",
    "        # Compute epoch statistics\n",
    "        avg_loss = total_loss / total_steps\n",
    "        min_loss = min(batch_losses)\n",
    "        max_loss = max(batch_losses)\n",
    "        \n",
    "        stats = {\n",
    "            'epoch': epoch + 1,\n",
    "            'avg_loss': avg_loss,\n",
    "            'min_loss': min_loss,\n",
    "            'max_loss': max_loss,\n",
    "            'total_steps': total_steps\n",
    "        }\n",
    "        epoch_stats.append(stats)\n",
    "        \n",
    "        print(\"\\nEpoch Statistics:\")\n",
    "        print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"Min Batch Loss: {min_loss:.4f}\")\n",
    "        print(f\"Max Batch Loss: {max_loss:.4f}\")\n",
    "        print(f\"Total Generation Steps: {total_steps}\")\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\nTraining Summary\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Epoch  |  Avg Loss  |  Min Loss  |  Max Loss\")\n",
    "    print(\"-\" * 40)\n",
    "    for stats in epoch_stats:\n",
    "        print(f\"{stats['epoch']:5d}  |  {stats['avg_loss']:.4f}    |  {stats['min_loss']:.4f}    |  {stats['max_loss']:.4f}\")\n",
    "\n",
    "\n",
    "        \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "# LoRA config\n",
    "LORA_RANK = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(base_model, lora_config).to(device)\n",
    "\n",
    "# Create generator\n",
    "generator = LoRAGenerator(\n",
    "    hidden_size=base_model.config.hidden_size,\n",
    "    lora_rank=LORA_RANK\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c934e01b58b54accb652b59665f9e51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1089 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader created successfully!\n",
      "Sample batch shape from dataloader:\n",
      "Context shape: torch.Size([4, 512])\n",
      "Target shape: torch.Size([4, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import Dataset as HFDataset\n",
    "import requests\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class ContextTargetDataset(Dataset):\n",
    "    def __init__(self, contexts, targets):\n",
    "        self.contexts = contexts\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.contexts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"context\": self.contexts[idx],\n",
    "            \"target\": self.targets[idx]\n",
    "        }\n",
    "\n",
    "def prepare_dataset(tokenizer, max_length=512, batch_size=4):\n",
    "    \"\"\"\n",
    "    Prepare dataset and create dataloader for LoRA training\n",
    "    \"\"\"\n",
    "    # Download tiny shakespeare dataset\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    response = requests.get(url)\n",
    "    text = response.text\n",
    "\n",
    "    # Split into chunks\n",
    "    chunk_size = max_length * 2  # To have room for both context and target\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size) if len(text[i:i+chunk_size]) == chunk_size]\n",
    "    \n",
    "    # Create dataset from chunks\n",
    "    data = [{\"text\": chunk} for chunk in chunks]\n",
    "    dataset = HFDataset.from_list(data)\n",
    "\n",
    "    # Initialize lists for contexts and targets\n",
    "    all_contexts = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Tokenization and processing function\n",
    "    def process_chunk(examples):\n",
    "        # Tokenize\n",
    "        tokenized = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=chunk_size,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Split into context and target\n",
    "        split_point = max_length\n",
    "        \n",
    "        for ids in tokenized[\"input_ids\"]:\n",
    "            context = ids[:split_point]\n",
    "            target = ids[split_point:split_point*2]\n",
    "            all_contexts.append(context)\n",
    "            all_targets.append(target)\n",
    "\n",
    "    # Process all examples\n",
    "    dataset.map(\n",
    "        process_chunk,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    # Create custom dataset\n",
    "    train_dataset = ContextTargetDataset(\n",
    "        contexts=torch.stack(all_contexts),\n",
    "        targets=torch.stack(all_targets)\n",
    "    )\n",
    "\n",
    "    # Create dataloader\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    return train_dataloader\n",
    "\n",
    "def setup_training(model_name=\"meta-llama/Llama-3.2-1B\", batch_size=4):\n",
    "    \"\"\"\n",
    "    Set up tokenizer and create dataloader\n",
    "    \"\"\"\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Set padding token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    # Create dataloader\n",
    "    train_dataloader = prepare_dataset(\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=512,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    return train_dataloader\n",
    "\n",
    "# Create dataloader\n",
    "train_dataloader = setup_training(batch_size=4)\n",
    "\n",
    "print(\"DataLoader created successfully!\")\n",
    "print(f\"Sample batch shape from dataloader:\")\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "print(f\"Context shape: {sample_batch['context'].shape}\")\n",
    "print(f\"Target shape: {sample_batch['target'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive_teacher_forcing(base_model, lora_model, input_ids, max_steps=20):\n",
    "    \"\"\"\n",
    "    Run step-by-step teacher forcing evaluation with gradient tracking\n",
    "    \"\"\"\n",
    "    device = input_ids.device\n",
    "    batch_size = input_ids.shape[0]\n",
    "    losses = []\n",
    "    \n",
    "    # Start with the input context\n",
    "    lora_tokens = input_ids.clone()\n",
    "    \n",
    "    # Get base model predictions first (without grad since we don't train base model)\n",
    "    with torch.no_grad():\n",
    "        base_outputs = base_model(input_ids=lora_tokens)\n",
    "        base_next_token = base_outputs.logits[:, -1, :].argmax(dim=-1)  # [batch_size]\n",
    "        base_probs = F.softmax(base_outputs.logits[:, -1, :], dim=-1)  # [batch_size, vocab_size]\n",
    "    \n",
    "    # Get LoRA predictions (with grad)\n",
    "    lora_outputs = lora_model(input_ids=lora_tokens)\n",
    "    lora_logits = lora_outputs.logits[:, -1, :]  # [batch_size, vocab_size]\n",
    "    lora_log_probs = F.log_softmax(lora_logits, dim=-1)  # [batch_size, vocab_size]\n",
    "    \n",
    "    # Compute KL divergence for this step\n",
    "    step_loss = F.kl_div(\n",
    "        lora_log_probs.unsqueeze(1),  # [batch, 1, vocab]\n",
    "        base_probs.unsqueeze(1),      # [batch, 1, vocab]\n",
    "        reduction='none'\n",
    "    ).sum(dim=-1)  # [batch, 1]\n",
    "    \n",
    "    return step_loss.mean()  # Return mean loss over batch\n",
    "\n",
    "def train_lora_generator(\n",
    "    base_model,\n",
    "    lora_model,\n",
    "    generator,\n",
    "    train_dataloader,\n",
    "    num_epochs=5,\n",
    "    learning_rate=1e-4,\n",
    "):\n",
    "    device = next(base_model.parameters()).device\n",
    "    generator = generator.to(device)\n",
    "    optimizer = torch.optim.AdamW(generator.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initialize target modules\n",
    "    target_modules = {}\n",
    "    for name, module in lora_model.named_modules():\n",
    "        if hasattr(module, 'lora_A'):\n",
    "            if 'q_proj' in name or 'v_proj' in name:\n",
    "                target_modules[name] = module\n",
    "    \n",
    "    print(f\"Found {len(target_modules)} target modules for LoRA updates\")\n",
    "    \n",
    "    # Track statistics\n",
    "    epoch_stats = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        total_loss = 0\n",
    "        batch_losses = []\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            context_ids = batch[\"context\"].to(device)\n",
    "            \n",
    "            # Get context embeddings for LoRA update generation\n",
    "            context_embeddings = base_model.model.embed_tokens(context_ids)\n",
    "            \n",
    "            # Generate and apply LoRA updates based on context\n",
    "            lora_updates = generator(context_embeddings)\n",
    "            \n",
    "            # Update LoRA weights\n",
    "            for module_name, update in lora_updates.items():\n",
    "                for full_name, module in target_modules.items():\n",
    "                    if module_name in full_name:\n",
    "                        lora_A = update['lora_A'].mean(dim=0)\n",
    "                        lora_B = update['lora_B'].mean(dim=0)\n",
    "                        module.lora_A[module.active_adapter].weight.data = lora_A\n",
    "                        module.lora_B[module.active_adapter].weight.data = lora_B\n",
    "            \n",
    "            # Run teacher forcing evaluation (with gradients)\n",
    "            loss = autoregressive_teacher_forcing(\n",
    "                base_model=base_model,\n",
    "                lora_model=lora_model,\n",
    "                input_ids=context_ids\n",
    "            )\n",
    "            \n",
    "            # Print sample predictions for first batch\n",
    "            if batch_idx == 0:\n",
    "                with torch.no_grad():\n",
    "                    base_out = base_model(input_ids=context_ids)\n",
    "                    lora_out = lora_model(input_ids=context_ids)\n",
    "                    base_next = base_out.logits[:, -1, :].argmax(dim=-1)\n",
    "                    lora_next = lora_out.logits[:, -1, :].argmax(dim=-1)\n",
    "                    print(\"\\nSample Predictions:\")\n",
    "                    print(\"Base model next token:\", tokenizer.decode(base_next[0]))\n",
    "                    print(\"LoRA model next token:\", tokenizer.decode(lora_next[0]))\n",
    "            \n",
    "            # Optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track statistics\n",
    "            current_loss = loss.item()\n",
    "            total_loss += current_loss\n",
    "            batch_losses.append(current_loss)\n",
    "            num_batches += 1\n",
    "            \n",
    "            print(f\"Batch {batch_idx+1}: Loss = {current_loss:.4f}\")\n",
    "        \n",
    "        # Compute epoch statistics\n",
    "        avg_loss = total_loss / num_batches\n",
    "        min_loss = min(batch_losses)\n",
    "        max_loss = max(batch_losses)\n",
    "        \n",
    "        stats = {\n",
    "            'epoch': epoch + 1,\n",
    "            'avg_loss': avg_loss,\n",
    "            'min_loss': min_loss,\n",
    "            'max_loss': max_loss,\n",
    "            'num_batches': num_batches\n",
    "        }\n",
    "        epoch_stats.append(stats)\n",
    "        \n",
    "        print(\"\\nEpoch Statistics:\")\n",
    "        print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"Min Loss: {min_loss:.4f}\")\n",
    "        print(f\"Max Loss: {max_loss:.4f}\")\n",
    "        print(f\"Number of Batches: {num_batches}\")\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\nTraining Summary\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Epoch  |  Avg Loss  |  Min Loss  |  Max Loss\")\n",
    "    print(\"-\" * 40)\n",
    "    for stats in epoch_stats:\n",
    "        print(f\"{stats['epoch']:5d}  |  {stats['avg_loss']:.4f}    |  {stats['min_loss']:.4f}    |  {stats['max_loss']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from datasets import Dataset as HFDataset\n",
    "import requests\n",
    "\n",
    "def prepare_mini_dataset(tokenizer, max_length=512, batch_size=4, num_samples=20):\n",
    "    \"\"\"\n",
    "    Prepare a small dataset for testing, using only num_samples examples\n",
    "    \"\"\"\n",
    "    # Download tiny shakespeare dataset\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    response = requests.get(url)\n",
    "    text = response.text\n",
    "\n",
    "    # Split into chunks\n",
    "    chunk_size = max_length * 2  # For both context and target\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size) \n",
    "             if len(text[i:i+chunk_size]) == chunk_size]\n",
    "    \n",
    "    # Take only the first num_samples chunks\n",
    "    chunks = chunks[:num_samples]\n",
    "    \n",
    "    # Create dataset\n",
    "    data = [{\"text\": chunk} for chunk in chunks]\n",
    "    dataset = HFDataset.from_list(data)\n",
    "\n",
    "    # Initialize lists for contexts and targets\n",
    "    all_contexts = []\n",
    "    all_targets = []\n",
    "\n",
    "    def process_chunk(examples):\n",
    "        tokenized = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=chunk_size,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Split each chunk into context and target\n",
    "        split_point = max_length\n",
    "        for ids in tokenized[\"input_ids\"]:\n",
    "            context = ids[:split_point]\n",
    "            target = ids[split_point:split_point*2]\n",
    "            all_contexts.append(context)\n",
    "            all_targets.append(target)\n",
    "\n",
    "    # Process examples\n",
    "    dataset.map(\n",
    "        process_chunk,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    # Create custom dataset\n",
    "    train_dataset = ContextTargetDataset(\n",
    "        contexts=torch.stack(all_contexts),\n",
    "        targets=torch.stack(all_targets)\n",
    "    )\n",
    "\n",
    "    print(f\"\\nDataset statistics:\")\n",
    "    print(f\"Number of samples: {len(train_dataset)}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Number of batches: {len(train_dataset) // batch_size}\")\n",
    "\n",
    "    return DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "class ContextTargetDataset(Dataset):\n",
    "    def __init__(self, contexts, targets):\n",
    "        self.contexts = contexts\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.contexts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"context\": self.contexts[idx],\n",
    "            \"target\": self.targets[idx]\n",
    "        }\n",
    "\n",
    "def setup_mini_training(model_name=\"meta-llama/Llama-3.2-1B\", batch_size=4, num_samples=20):\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    # Create mini dataloader\n",
    "    train_dataloader = prepare_mini_dataset(\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=512,\n",
    "        batch_size=batch_size,\n",
    "        num_samples=num_samples\n",
    "    )\n",
    "    \n",
    "    return train_dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09db418311cd4c1296a21dd87e03c780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset statistics:\n",
      "Number of samples: 50\n",
      "Batch size: 10\n",
      "Number of batches: 5\n",
      "Found 32 target modules for LoRA updates\n",
      "\n",
      "Epoch 1/25\n",
      "--------------------\n",
      "\n",
      "Sample Predictions:\n",
      "Base model next token: oth\n",
      "LoRA model next token: oth\n",
      "Batch 1: Loss = 0.0831\n",
      "Batch 2: Loss = 0.0557\n",
      "Batch 3: Loss = 0.1057\n",
      "Batch 4: Loss = 0.1141\n",
      "Batch 5: Loss = 0.2575\n",
      "\n",
      "Epoch Statistics:\n",
      "Average Loss: 0.1232\n",
      "Min Loss: 0.0557\n",
      "Max Loss: 0.2575\n",
      "Number of Batches: 5\n",
      "\n",
      "Epoch 2/25\n",
      "--------------------\n",
      "\n",
      "Sample Predictions:\n",
      "Base model next token: oth\n",
      "LoRA model next token: oth\n",
      "Batch 1: Loss = 0.0886\n",
      "Batch 2: Loss = 0.1606\n",
      "Batch 3: Loss = 0.0614\n",
      "Batch 4: Loss = 0.2366\n",
      "Batch 5: Loss = 0.0533\n",
      "\n",
      "Epoch Statistics:\n",
      "Average Loss: 0.1201\n",
      "Min Loss: 0.0533\n",
      "Max Loss: 0.2366\n",
      "Number of Batches: 5\n",
      "\n",
      "Epoch 3/25\n",
      "--------------------\n",
      "\n",
      "Sample Predictions:\n",
      "Base model next token: oth\n",
      "LoRA model next token: oth\n",
      "Batch 1: Loss = 0.3388\n",
      "Batch 2: Loss = 0.1888\n",
      "Batch 3: Loss = 0.2961\n",
      "Batch 4: Loss = 0.1414\n",
      "Batch 5: Loss = 0.0686\n",
      "\n",
      "Epoch Statistics:\n",
      "Average Loss: 0.2067\n",
      "Min Loss: 0.0686\n",
      "Max Loss: 0.3388\n",
      "Number of Batches: 5\n",
      "\n",
      "Epoch 4/25\n",
      "--------------------\n",
      "\n",
      "Sample Predictions:\n",
      "Base model next token: oth\n",
      "LoRA model next token: oth\n",
      "Batch 1: Loss = 0.0871\n",
      "Batch 2: Loss = 0.2005\n",
      "Batch 3: Loss = 0.1379\n",
      "Batch 4: Loss = 0.4515\n",
      "Batch 5: Loss = 0.0490\n",
      "\n",
      "Epoch Statistics:\n",
      "Average Loss: 0.1852\n",
      "Min Loss: 0.0490\n",
      "Max Loss: 0.4515\n",
      "Number of Batches: 5\n",
      "\n",
      "Epoch 5/25\n",
      "--------------------\n",
      "\n",
      "Sample Predictions:\n",
      "Base model next token: oth\n",
      "LoRA model next token: oth\n",
      "Batch 1: Loss = 0.1451\n",
      "Batch 2: Loss = 0.0534\n",
      "Batch 3: Loss = 0.0746\n",
      "Batch 4: Loss = 0.1000\n",
      "Batch 5: Loss = 0.0260\n",
      "\n",
      "Epoch Statistics:\n",
      "Average Loss: 0.0798\n",
      "Min Loss: 0.0260\n",
      "Max Loss: 0.1451\n",
      "Number of Batches: 5\n",
      "\n",
      "Epoch 6/25\n",
      "--------------------\n",
      "\n",
      "Sample Predictions:\n",
      "Base model next token: oth\n",
      "LoRA model next token: oth\n",
      "Batch 1: Loss = 0.0789\n",
      "Batch 2: Loss = 0.0569\n",
      "Batch 3: Loss = 0.0755\n",
      "Batch 4: Loss = 0.2099\n",
      "Batch 5: Loss = 0.0668\n",
      "\n",
      "Epoch Statistics:\n",
      "Average Loss: 0.0976\n",
      "Min Loss: 0.0569\n",
      "Max Loss: 0.2099\n",
      "Number of Batches: 5\n",
      "\n",
      "Epoch 7/25\n",
      "--------------------\n",
      "\n",
      "Sample Predictions:\n",
      "Base model next token:  peoples\n",
      "LoRA model next token: oth\n",
      "Batch 1: Loss = 0.0582\n",
      "Batch 2: Loss = 0.1909\n",
      "Batch 3: Loss = 0.0808\n",
      "Batch 4: Loss = 0.0455\n",
      "Batch 5: Loss = 0.0896\n",
      "\n",
      "Epoch Statistics:\n",
      "Average Loss: 0.0930\n",
      "Min Loss: 0.0455\n",
      "Max Loss: 0.1909\n",
      "Number of Batches: 5\n",
      "\n",
      "Epoch 8/25\n",
      "--------------------\n",
      "\n",
      "Sample Predictions:\n",
      "Base model next token:  FIG\n",
      "LoRA model next token: oth\n",
      "Batch 1: Loss = 0.0664\n",
      "Batch 2: Loss = 0.1191\n",
      "Batch 3: Loss = 0.1545\n",
      "Batch 4: Loss = 0.0677\n",
      "Batch 5: Loss = 0.3487\n",
      "\n",
      "Epoch Statistics:\n",
      "Average Loss: 0.1513\n",
      "Min Loss: 0.0664\n",
      "Max Loss: 0.3487\n",
      "Number of Batches: 5\n",
      "\n",
      "Epoch 9/25\n",
      "--------------------\n",
      "\n",
      "Sample Predictions:\n",
      "Base model next token: oth\n",
      "LoRA model next token: oth\n",
      "Batch 1: Loss = 0.0808\n",
      "Batch 2: Loss = 0.0954\n",
      "Batch 3: Loss = 0.0976\n",
      "Batch 4: Loss = 0.0863\n",
      "Batch 5: Loss = 0.1645\n",
      "\n",
      "Epoch Statistics:\n",
      "Average Loss: 0.1049\n",
      "Min Loss: 0.0808\n",
      "Max Loss: 0.1645\n",
      "Number of Batches: 5\n",
      "\n",
      "Epoch 10/25\n",
      "--------------------\n",
      "\n",
      "Sample Predictions:\n",
      "Base model next token: oth\n",
      "LoRA model next token: oth\n",
      "Batch 1: Loss = 0.1706\n",
      "Batch 2: Loss = 0.1183\n",
      "Batch 3: Loss = 0.0412\n",
      "Batch 4: Loss = 0.0645\n",
      "Batch 5: Loss = 0.0960\n",
      "\n",
      "Epoch Statistics:\n",
      "Average Loss: 0.0981\n",
      "Min Loss: 0.0412\n",
      "Max Loss: 0.1706\n",
      "Number of Batches: 5\n",
      "\n",
      "Epoch 11/25\n",
      "--------------------\n",
      "\n",
      "Sample Predictions:\n",
      "Base model next token:  peoples\n",
      "LoRA model next token: oth\n",
      "Batch 1: Loss = 0.0818\n",
      "Batch 2: Loss = 0.1273\n",
      "Batch 3: Loss = 0.2839\n",
      "Batch 4: Loss = 0.0521\n",
      "Batch 5: Loss = 0.0786\n",
      "\n",
      "Epoch Statistics:\n",
      "Average Loss: 0.1247\n",
      "Min Loss: 0.0521\n",
      "Max Loss: 0.2839\n",
      "Number of Batches: 5\n",
      "\n",
      "Epoch 12/25\n",
      "--------------------\n",
      "\n",
      "Sample Predictions:\n",
      "Base model next token: oth\n",
      "LoRA model next token:  `_\n",
      "Batch 1: Loss = 0.0409\n",
      "Batch 2: Loss = 0.1018\n",
      "Batch 3: Loss = 0.0466\n",
      "Batch 4: Loss = 0.0452\n",
      "Batch 5: Loss = 0.0596\n",
      "\n",
      "Epoch Statistics:\n",
      "Average Loss: 0.0588\n",
      "Min Loss: 0.0409\n",
      "Max Loss: 0.1018\n",
      "Number of Batches: 5\n",
      "\n",
      "Epoch 13/25\n",
      "--------------------\n",
      "\n",
      "Sample Predictions:\n",
      "Base model next token: oth\n",
      "LoRA model next token: oth\n",
      "Batch 1: Loss = 0.1609\n",
      "Batch 2: Loss = 0.1108\n",
      "Batch 3: Loss = 0.2237\n",
      "Batch 4: Loss = 0.0582\n",
      "Batch 5: Loss = 0.0328\n",
      "\n",
      "Epoch Statistics:\n",
      "Average Loss: 0.1173\n",
      "Min Loss: 0.0328\n",
      "Max Loss: 0.2237\n",
      "Number of Batches: 5\n",
      "\n",
      "Epoch 14/25\n",
      "--------------------\n",
      "\n",
      "Sample Predictions:\n",
      "Base model next token: oth\n",
      "LoRA model next token: oth\n",
      "Batch 1: Loss = 0.1031\n",
      "Batch 2: Loss = 0.0659\n",
      "Batch 3: Loss = 0.0827\n",
      "Batch 4: Loss = 0.0742\n",
      "Batch 5: Loss = 0.0667\n",
      "\n",
      "Epoch Statistics:\n",
      "Average Loss: 0.0785\n",
      "Min Loss: 0.0659\n",
      "Max Loss: 0.1031\n",
      "Number of Batches: 5\n",
      "\n",
      "Epoch 15/25\n",
      "--------------------\n",
      "\n",
      "Sample Predictions:\n",
      "Base model next token: oth\n",
      "LoRA model next token: oth\n",
      "Batch 1: Loss = 0.0588\n",
      "Batch 2: Loss = 0.2242\n",
      "Batch 3: Loss = 0.1507\n",
      "Batch 4: Loss = 0.0490\n",
      "Batch 5: Loss = 0.0627\n",
      "\n",
      "Epoch Statistics:\n",
      "Average Loss: 0.1091\n",
      "Min Loss: 0.0490\n",
      "Max Loss: 0.2242\n",
      "Number of Batches: 5\n",
      "\n",
      "Epoch 16/25\n",
      "--------------------\n",
      "\n",
      "Sample Predictions:\n",
      "Base model next token:  `_\n",
      "LoRA model next token: oth\n",
      "Batch 1: Loss = 0.0758\n",
      "Batch 2: Loss = 0.1786\n",
      "Batch 3: Loss = 0.0850\n",
      "Batch 4: Loss = 0.0515\n",
      "Batch 5: Loss = 0.1574\n",
      "\n",
      "Epoch Statistics:\n",
      "Average Loss: 0.1096\n",
      "Min Loss: 0.0515\n",
      "Max Loss: 0.1786\n",
      "Number of Batches: 5\n",
      "\n",
      "Epoch 17/25\n",
      "--------------------\n",
      "\n",
      "Sample Predictions:\n",
      "Base model next token: oth\n",
      "LoRA model next token:  `_\n",
      "Batch 1: Loss = 0.0447\n",
      "Batch 2: Loss = 0.1275\n",
      "Batch 3: Loss = 0.0670\n",
      "Batch 4: Loss = 0.0908\n",
      "Batch 5: Loss = 0.1456\n",
      "\n",
      "Epoch Statistics:\n",
      "Average Loss: 0.0952\n",
      "Min Loss: 0.0447\n",
      "Max Loss: 0.1456\n",
      "Number of Batches: 5\n",
      "\n",
      "Epoch 18/25\n",
      "--------------------\n",
      "\n",
      "Sample Predictions:\n",
      "Base model next token: oth\n",
      "LoRA model next token: oth\n",
      "Batch 1: Loss = 0.2882\n",
      "Batch 2: Loss = 0.1127\n",
      "Batch 3: Loss = 0.0596\n",
      "Batch 4: Loss = 0.0662\n",
      "Batch 5: Loss = 0.2053\n",
      "\n",
      "Epoch Statistics:\n",
      "Average Loss: 0.1464\n",
      "Min Loss: 0.0596\n",
      "Max Loss: 0.2882\n",
      "Number of Batches: 5\n",
      "\n",
      "Epoch 19/25\n",
      "--------------------\n",
      "\n",
      "Sample Predictions:\n",
      "Base model next token:  peoples\n",
      "LoRA model next token:  `_\n",
      "Batch 1: Loss = 0.2648\n",
      "Batch 2: Loss = 0.1623\n",
      "Batch 3: Loss = 0.1721\n",
      "Batch 4: Loss = 0.1621\n",
      "Batch 5: Loss = 0.0632\n",
      "\n",
      "Epoch Statistics:\n",
      "Average Loss: 0.1649\n",
      "Min Loss: 0.0632\n",
      "Max Loss: 0.2648\n",
      "Number of Batches: 5\n",
      "\n",
      "Epoch 20/25\n",
      "--------------------\n",
      "\n",
      "Sample Predictions:\n",
      "Base model next token: oth\n",
      "LoRA model next token: oth\n",
      "Batch 1: Loss = 0.0490\n",
      "Batch 2: Loss = 0.0637\n",
      "Batch 3: Loss = 0.1332\n",
      "Batch 4: Loss = 0.0905\n",
      "Batch 5: Loss = 0.0945\n",
      "\n",
      "Epoch Statistics:\n",
      "Average Loss: 0.0862\n",
      "Min Loss: 0.0490\n",
      "Max Loss: 0.1332\n",
      "Number of Batches: 5\n",
      "\n",
      "Epoch 21/25\n",
      "--------------------\n",
      "\n",
      "Sample Predictions:\n",
      "Base model next token: oth\n",
      "LoRA model next token: oth\n",
      "Batch 1: Loss = 0.0698\n",
      "Batch 2: Loss = 0.1093\n",
      "Batch 3: Loss = 0.0554\n",
      "Batch 4: Loss = 0.0880\n",
      "Batch 5: Loss = 0.1412\n",
      "\n",
      "Epoch Statistics:\n",
      "Average Loss: 0.0927\n",
      "Min Loss: 0.0554\n",
      "Max Loss: 0.1412\n",
      "Number of Batches: 5\n",
      "\n",
      "Epoch 22/25\n",
      "--------------------\n",
      "\n",
      "Sample Predictions:\n",
      "Base model next token: oth\n",
      "LoRA model next token: oth\n",
      "Batch 1: Loss = 0.1393\n",
      "Batch 2: Loss = 0.0602\n",
      "Batch 3: Loss = 0.3233\n",
      "Batch 4: Loss = 0.2190\n",
      "Batch 5: Loss = 0.1059\n",
      "\n",
      "Epoch Statistics:\n",
      "Average Loss: 0.1695\n",
      "Min Loss: 0.0602\n",
      "Max Loss: 0.3233\n",
      "Number of Batches: 5\n",
      "\n",
      "Epoch 23/25\n",
      "--------------------\n",
      "\n",
      "Sample Predictions:\n",
      "Base model next token: oth\n",
      "LoRA model next token: oth\n",
      "Batch 1: Loss = 0.0811\n",
      "Batch 2: Loss = 0.0720\n",
      "Batch 3: Loss = 0.1238\n",
      "Batch 4: Loss = 0.0687\n",
      "Batch 5: Loss = 0.0929\n",
      "\n",
      "Epoch Statistics:\n",
      "Average Loss: 0.0877\n",
      "Min Loss: 0.0687\n",
      "Max Loss: 0.1238\n",
      "Number of Batches: 5\n",
      "\n",
      "Epoch 24/25\n",
      "--------------------\n",
      "\n",
      "Sample Predictions:\n",
      "Base model next token: oth\n",
      "LoRA model next token: oth\n",
      "Batch 1: Loss = 0.0457\n",
      "Batch 2: Loss = 0.1444\n",
      "Batch 3: Loss = 0.1823\n",
      "Batch 4: Loss = 0.0518\n",
      "Batch 5: Loss = 0.1740\n",
      "\n",
      "Epoch Statistics:\n",
      "Average Loss: 0.1196\n",
      "Min Loss: 0.0457\n",
      "Max Loss: 0.1823\n",
      "Number of Batches: 5\n",
      "\n",
      "Epoch 25/25\n",
      "--------------------\n",
      "\n",
      "Sample Predictions:\n",
      "Base model next token:  peoples\n",
      "LoRA model next token: oth\n",
      "Batch 1: Loss = 0.0461\n",
      "Batch 2: Loss = 0.0893\n",
      "Batch 3: Loss = 0.0921\n",
      "Batch 4: Loss = 0.2149\n",
      "Batch 5: Loss = 0.0455\n",
      "\n",
      "Epoch Statistics:\n",
      "Average Loss: 0.0976\n",
      "Min Loss: 0.0455\n",
      "Max Loss: 0.2149\n",
      "Number of Batches: 5\n",
      "\n",
      "Training Summary\n",
      "========================================\n",
      "Epoch  |  Avg Loss  |  Min Loss  |  Max Loss\n",
      "----------------------------------------\n",
      "    1  |  0.1232    |  0.0557    |  0.2575\n",
      "    2  |  0.1201    |  0.0533    |  0.2366\n",
      "    3  |  0.2067    |  0.0686    |  0.3388\n",
      "    4  |  0.1852    |  0.0490    |  0.4515\n",
      "    5  |  0.0798    |  0.0260    |  0.1451\n",
      "    6  |  0.0976    |  0.0569    |  0.2099\n",
      "    7  |  0.0930    |  0.0455    |  0.1909\n",
      "    8  |  0.1513    |  0.0664    |  0.3487\n",
      "    9  |  0.1049    |  0.0808    |  0.1645\n",
      "   10  |  0.0981    |  0.0412    |  0.1706\n",
      "   11  |  0.1247    |  0.0521    |  0.2839\n",
      "   12  |  0.0588    |  0.0409    |  0.1018\n",
      "   13  |  0.1173    |  0.0328    |  0.2237\n",
      "   14  |  0.0785    |  0.0659    |  0.1031\n",
      "   15  |  0.1091    |  0.0490    |  0.2242\n",
      "   16  |  0.1096    |  0.0515    |  0.1786\n",
      "   17  |  0.0952    |  0.0447    |  0.1456\n",
      "   18  |  0.1464    |  0.0596    |  0.2882\n",
      "   19  |  0.1649    |  0.0632    |  0.2648\n",
      "   20  |  0.0862    |  0.0490    |  0.1332\n",
      "   21  |  0.0927    |  0.0554    |  0.1412\n",
      "   22  |  0.1695    |  0.0602    |  0.3233\n",
      "   23  |  0.0877    |  0.0687    |  0.1238\n",
      "   24  |  0.1196    |  0.0457    |  0.1823\n",
      "   25  |  0.0976    |  0.0455    |  0.2149\n"
     ]
    }
   ],
   "source": [
    "# Create dataloader and start training\n",
    "# train_dataloader = setup_training(batch_size=4)\n",
    "train_dataloader = setup_mini_training(batch_size=10, num_samples=50)  # 5 batches total\n",
    "\n",
    "# Run training with statistics\n",
    "train_lora_generator(\n",
    "    base_model=base_model,\n",
    "    lora_model=lora_model,\n",
    "    generator=generator,\n",
    "    train_dataloader=train_dataloader,\n",
    "    num_epochs=25,\n",
    "    learning_rate=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Module: base_model.model.model.layers.0.self_attn.q_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([2048, 8])\n",
      "Weight shape: torch.Size([2048, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.0.self_attn.v_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([512, 8])\n",
      "Weight shape: torch.Size([512, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.1.self_attn.q_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([2048, 8])\n",
      "Weight shape: torch.Size([2048, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.1.self_attn.v_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([512, 8])\n",
      "Weight shape: torch.Size([512, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.2.self_attn.q_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([2048, 8])\n",
      "Weight shape: torch.Size([2048, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.2.self_attn.v_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([512, 8])\n",
      "Weight shape: torch.Size([512, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.3.self_attn.q_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([2048, 8])\n",
      "Weight shape: torch.Size([2048, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.3.self_attn.v_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([512, 8])\n",
      "Weight shape: torch.Size([512, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.4.self_attn.q_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([2048, 8])\n",
      "Weight shape: torch.Size([2048, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.4.self_attn.v_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([512, 8])\n",
      "Weight shape: torch.Size([512, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.5.self_attn.q_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([2048, 8])\n",
      "Weight shape: torch.Size([2048, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.5.self_attn.v_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([512, 8])\n",
      "Weight shape: torch.Size([512, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.6.self_attn.q_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([2048, 8])\n",
      "Weight shape: torch.Size([2048, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.6.self_attn.v_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([512, 8])\n",
      "Weight shape: torch.Size([512, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.7.self_attn.q_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([2048, 8])\n",
      "Weight shape: torch.Size([2048, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.7.self_attn.v_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([512, 8])\n",
      "Weight shape: torch.Size([512, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.8.self_attn.q_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([2048, 8])\n",
      "Weight shape: torch.Size([2048, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.8.self_attn.v_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([512, 8])\n",
      "Weight shape: torch.Size([512, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.9.self_attn.q_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([2048, 8])\n",
      "Weight shape: torch.Size([2048, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.9.self_attn.v_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([512, 8])\n",
      "Weight shape: torch.Size([512, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.10.self_attn.q_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([2048, 8])\n",
      "Weight shape: torch.Size([2048, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.10.self_attn.v_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([512, 8])\n",
      "Weight shape: torch.Size([512, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.11.self_attn.q_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([2048, 8])\n",
      "Weight shape: torch.Size([2048, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.11.self_attn.v_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([512, 8])\n",
      "Weight shape: torch.Size([512, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.12.self_attn.q_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([2048, 8])\n",
      "Weight shape: torch.Size([2048, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.12.self_attn.v_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([512, 8])\n",
      "Weight shape: torch.Size([512, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.13.self_attn.q_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([2048, 8])\n",
      "Weight shape: torch.Size([2048, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.13.self_attn.v_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([512, 8])\n",
      "Weight shape: torch.Size([512, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.14.self_attn.q_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([2048, 8])\n",
      "Weight shape: torch.Size([2048, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.14.self_attn.v_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([512, 8])\n",
      "Weight shape: torch.Size([512, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.15.self_attn.q_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([2048, 8])\n",
      "Weight shape: torch.Size([2048, 2048])\n",
      "\n",
      "Module: base_model.model.model.layers.15.self_attn.v_proj\n",
      "LoRA A shape: torch.Size([8, 2048])\n",
      "LoRA B shape: torch.Size([512, 8])\n",
      "Weight shape: torch.Size([512, 2048])\n",
      "\n",
      "Shape Summary:\n",
      "\n",
      "Unique shape patterns:\n",
      "\n",
      "Shapes ('A:torch.Size([8, 2048])', 'B:torch.Size([2048, 8])') found in modules: ['self_attn', 'self_attn', 'self_attn', 'self_attn', 'self_attn', 'self_attn', 'self_attn', 'self_attn', 'self_attn', 'self_attn', 'self_attn', 'self_attn', 'self_attn', 'self_attn', 'self_attn', 'self_attn']\n",
      "\n",
      "Shapes ('A:torch.Size([8, 2048])', 'B:torch.Size([512, 8])') found in modules: ['self_attn', 'self_attn', 'self_attn', 'self_attn', 'self_attn', 'self_attn', 'self_attn', 'self_attn', 'self_attn', 'self_attn', 'self_attn', 'self_attn', 'self_attn', 'self_attn', 'self_attn', 'self_attn']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Setup base models first\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\").to(device)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# Create LoRA model\n",
    "lora_model = get_peft_model(base_model, lora_config).to(device)\n",
    "\n",
    "# Inspect shapes\n",
    "shapes = {}\n",
    "for name, module in lora_model.named_modules():\n",
    "    if hasattr(module, 'lora_A'):\n",
    "        shapes[name] = {\n",
    "            'lora_A': module.lora_A[module.active_adapter].weight.shape,\n",
    "            'lora_B': module.lora_B[module.active_adapter].weight.shape,\n",
    "            'weight': module.weight.shape if hasattr(module, 'weight') else None\n",
    "        }\n",
    "        print(f\"\\nModule: {name}\")\n",
    "        print(f\"LoRA A shape: {shapes[name]['lora_A']}\")\n",
    "        print(f\"LoRA B shape: {shapes[name]['lora_B']}\")\n",
    "        print(f\"Weight shape: {shapes[name]['weight']}\")\n",
    "\n",
    "print(\"\\nShape Summary:\")\n",
    "unique_shapes = {}\n",
    "for name, shape_info in shapes.items():\n",
    "    key = (f\"A:{shape_info['lora_A']}\", f\"B:{shape_info['lora_B']}\")\n",
    "    if key not in unique_shapes:\n",
    "        unique_shapes[key] = []\n",
    "    unique_shapes[key].append(name.split('.')[-2])  # Get the module type (q_proj or v_proj)\n",
    "\n",
    "print(\"\\nUnique shape patterns:\")\n",
    "for shapes, modules in unique_shapes.items():\n",
    "    print(f\"\\nShapes {shapes} found in modules: {modules}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jshen3Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
