{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vivekvajipey/miniconda3/envs/reasoning/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/vivekvajipey/miniconda3/envs/reasoning/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <5AA8DD3D-A2CC-31CA-8060-88B4E9C18B09> /Users/vivekvajipey/miniconda3/envs/reasoning/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <3FE16EFD-B5EE-33D2-A4C8-2BD93F2AA7BA> /Users/vivekvajipey/miniconda3/envs/reasoning/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/Users/vivekvajipey/miniconda3/envs/reasoning/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/Users/vivekvajipey/miniconda3/envs/reasoning/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "First half: Cheese is a type of dairy product produced in a range of flavors, textures, and forms by coagulation of the milk protein casein. It comprises proteins and fat from milk (usually the milk of cows, buffalo, goats or sheep). During production, milk is usually acidified and either\n",
      "\n",
      "Second half:  the enzymes of rennet or bacterial enzymes with similar activity are added to cause the casein to coagulate. The solid curds are then separated from the liquid whey and pressed into finished cheese. Some cheeses have aromatic molds on the rind, the outer layer, or throughout.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Setup and Imports\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Check device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Step 2: Prepare Document\n",
    "document = \"\"\"Cheese is a type of dairy product produced in a range of flavors, textures, and forms by coagulation of the milk protein casein. It comprises proteins and fat from milk (usually the milk of cows, buffalo, goats or sheep). During production, milk is usually acidified and either the enzymes of rennet or bacterial enzymes with similar activity are added to cause the casein to coagulate. The solid curds are then separated from the liquid whey and pressed into finished cheese. Some cheeses have aromatic molds on the rind, the outer layer, or throughout.\"\"\"\n",
    "\n",
    "# Split document\n",
    "split_idx = len(document) // 2\n",
    "first_half = document[:split_idx]\n",
    "second_half = document[split_idx:]\n",
    "\n",
    "print(\"First half:\", first_half)\n",
    "print(\"\\nSecond half:\", second_half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9988a57ffbcd4a2abebc50d148187a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer initialized\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Initialize Model and Tokenizer\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set model dtype based on device\n",
    "dtype = torch.float16 if device.type == \"cuda\" else torch.float32\n",
    "\n",
    "# Initialize model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Model and tokenizer initialized\")\n",
    "\n",
    "# Step 4: Configure and Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): all input tensors must be on the same device. Received mps:0 and cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m bos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[tokenizer\u001b[38;5;241m.\u001b[39mbos_token_id]], device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Prepare input: [start_token, token1, ..., token(n-1)]\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Labels: [token1, ..., token(n)]\u001b[39;00m\n\u001b[1;32m     16\u001b[0m labels \u001b[38;5;241m=\u001b[39m token_ids\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.cat(): all input tensors must be on the same device. Received mps:0 and cpu"
     ]
    }
   ],
   "source": [
    "# Step 5: Prepare Training Data\n",
    "# Tokenize second half (setting max_length to avoid truncation warning)\n",
    "second_half_tokens = tokenizer(second_half, return_tensors=\"pt\", max_length=512)\n",
    "token_ids = second_half_tokens[\"input_ids\"]\n",
    "\n",
    "# Create single start token tensor\n",
    "bos = torch.tensor([[tokenizer.bos_token_id]], device=device)\n",
    "\n",
    "# Prepare input: [start_token, token1, ..., token(n-1)]\n",
    "input_ids = torch.cat([\n",
    "    bos,\n",
    "    token_ids[:, :-1]\n",
    "], dim=1).to(device)\n",
    "\n",
    "# Labels: [token1, ..., token(n)]\n",
    "labels = token_ids.to(device)\n",
    "\n",
    "print(\"\\nShapes:\")\n",
    "print(\"Input shape:\", input_ids.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "print(\"\\nTokens:\")\n",
    "print(\"First few input tokens:\", tokenizer.convert_ids_to_tokens(input_ids[0])[:10])\n",
    "print(\"First few label tokens:\", tokenizer.convert_ids_to_tokens(labels[0])[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "once again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "First half: Cheese is a type of dairy product produced in a range of flavors, textures, and forms by coagulation of the milk protein casein. It comprises proteins and fat from milk (usually the milk of cows, buffalo, goats or sheep). During production, milk is usually acidified and either\n",
      "\n",
      "Second half:  the enzymes of rennet or bacterial enzymes with similar activity are added to cause the casein to coagulate. The solid curds are then separated from the liquid whey and pressed into finished cheese. Some cheeses have aromatic molds on the rind, the outer layer, or throughout.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73c0f2d7e144d7183a455e5284974f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer initialized\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): all input tensors must be on the same device. Received mps:0 and cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m bos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[tokenizer\u001b[38;5;241m.\u001b[39mbos_token_id]], device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Prepare input: [start_token, token1, ..., token(n-1)]\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Labels: [token1, ..., token(n)]\u001b[39;00m\n\u001b[1;32m     76\u001b[0m labels \u001b[38;5;241m=\u001b[39m token_ids\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.cat(): all input tensors must be on the same device. Received mps:0 and cpu"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasoning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
